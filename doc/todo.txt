= TODO =

== Documentation of Quirks and Stuff That Future Programmers Must Know ==

- PRIORITY document TaskComplete handling...
- PRIORITY document SkeletonBTreeMap reliance on isLeaf() and size() being
  called in the correct places.

== Index semantics ==

- PRIORITY work out semantics for getting a subset of a collection of entries

- TODO index: "suggested tokens" - suggest what other people should tag this
  index as; they are free to ignore this suggestion (very low priority for now)


== Task handling ==

- TODO work out TaskAbortException handling for SkeletonTreeMap
- TODO maybe have deflate/inflate throw a different checked exception from
  TaskAbortException, forcing them to handle the TaskAbortException thrown by
  pull/push?

== TokenEntry semantics ==

- TODO implement equals() and compare() on TokenEntry properly
- TODO there should be only one entry per URI per Token (enforce this in the code at some point)
- TODO make TokenEntries immutable, and make Yaml stuff to create/uncreate these...
- PRIORITY merge URIWrapper with TokenURIEntry

== Request/Progress reporting ==

- PRIORITY make a CompoundRequest that extends CompoundProgress (and make CompondProgress better and more standard)
  - and it will have equivalent functionality as "getSubRequests"
  - how to implement getSubStage* in Search, and put it back into FindRequest
- implement nice version of finalTotalEstimate() in FindRequest and Search
- TODO probably should synchronize AtomicProgress, SimpleProgress, and
  CompoundProgress... although maybe unnecessary since these should all be
  single-writer multi-reader (and the reads are not critical)?

== Packaging structure ==

- PRIORITY do we want to merge the library/ and index/ subpackages? or shuffle
things around?
- TODO get rid of unnecessary Prefix* files
- TODO restructure the Serialiser.* stuff
- TODO refactor this, maybe..?
  - have a Skeleton<K> interface with deflate(K) methods so that
  SkeletonMap<K, V> extends Skeleton<K>


== Misc ==

- TODO use markdown-doclet instead of bliki-doclet
  - not yet - MarkdownJ has bugs relating to parsing of uppercase HTML tags,
    which SJD generates, unfortunately :/
- TODO make build.xml output test results better... atm either it has two modes:
  - print test stdout output out in real time, but not stderr
  - print stdout and stderr in a single go after the test is over :/





>>> 7f0a7773dc3d60759145e10b750e706c584d8a7c
>>>
>>> URIKey: - I believe we agreed to index by the concatenation of the
>>> routing key and the uri hash? is that the current plan? you could index
>>> by the uri (toString(false, true) -> ascii), but that wouldn't be fixed
>>> length. i dunno how much locality of reference matters within a given
>>> routing key, if it does you need to either index by the actual uri or
>>> use a locality-preserving hash such as tea hash.
>> i'll need to rethink this at some point. the problem with just
>> concatenation is that you'll end up with a tree that's (length of the
>> first part) deep, with nodes in between that only have 1 children. so atm
>> i've gone with only routingKey (probably will soon change this to
>> routingKey + docName, as SSK uses). it will index small-to-medium sites
>> fine, but will be bad for extremely large (thousands of pages) sites.
>> maybe i could use skip-lists instead of a tree within a tree
>> (http://en.wikipedia.org/wiki/Skip_list).
>
> Maybe, I'm not sure how you'd lazily load a skiplist? I guess you can
> segment each level.
>
> Alternatively, what about allowing a child with a prefix longer than [ my
> prefix ] + 1, provided that all keys in that bucket (with that next-element)
> fit within it? And if and when one is added, create an intermediate subtree?
> Of course this means changing the parent pointer when you move the tree
> downwards, but is that such a big deal? You could have a protected or
> package-local accessor...

it would be easier to just grab a B-tree implementation and modify it to suit
our purposes, lol (and we wouldn't have to hash them into Tokens)... actually i
might do just that instead of coding a custom algorithm. atm though i'm going
to carry on other parts of the project, i can mess about with fine-tuning the
data structure more after the rest is working.



== Schedule ==

 - URI map, 1-2 days
 - implement freenet inflate/deflate 1-2 days
 - tidying up index stuff 2-3 days

 - build interdex skeleton 2-3 days
 - implement interdex algorithm 4-5 days

--- gsoc end ---

 - implement filters 2 days
 - implement WriteableIndex and commit algorithms 4-6 days
 - implement crawler...



