= Index =

== TODO ==

- can have terms containing spaces by eg. having crawler add "x y" to its dictionary of
terms to index, if it encounters this combination more than a few times on multiple pages.

- notes for synchronization:

read operations should be able to occur concurrently

put/remove modify the map structurally.
when this is happening, no other operations can happen

deflate/inflate modify the map data but not structure.
deflate/inflate should be able to occur concurrently on different nodes/keys
it should block put/remove operations, but not read operations


=== Index commit algorithm ===

have a "modCount" for each node. merge/split/rotate & put/remove update this

WriteableIndex keeps a "node -> modCount" map
at each commit, push all nodes that have a different modCount,
and update the map.

==== Parallel BTreeMap bulk-merge algorithm ====

life cycle of a node:

- node gets popped from inflated
- enter inflate_subnodes
  - subnodes get pushed into pull_queue
  - (recurse for each subnode)
    - subnodes get popped from inflated
    - etc
    - subnodes get pushed into push_queue
- wait for all:
  - subnodes get popped from deflated
- enter split
  - for each item in the original node's value_clo
    - release the item and acquire it on
      - the parent's value_clo if the item is a separator
      - a new value_clo if the item is now in a split_node
- for each split_node:
  - wait for all:
    - values get popped from values_complete
  - enter deflate_splitnode
    - split_nodes gets pushed into push_queue (needs exception for the root node)


!!! PriorityQueues must produce nodes in depth-first order !!!

execute(SortedMap<K, V> map):

	final PriorityBlockingQueue<PullTask<Node>> pull_queue // input queue for pull-scheduler
	final PriorityBlockingQueue<PullTask<Node>> inflated // output queue for pull-scheduler
	// FIXME error maps

	final PriorityBlockingQueue<PushTask<Node>> push_queue // input queue for push-scheduler
	final PriorityBlockingQueue<PushTask<Node>> deflated // output queue for push-scheduler
	// FIXME error maps

	final PriorityBlockingQueue<K> value_pending // input queue for value-handler
	final PriorityBlockingQueue<K> value_complete // output queue for value-handler
	// FIXME correct type instead of <K>
	// FIXME error maps

	final IDMap<PullTask<Node>, Closure<Node>> inflate_closures
	final IDMap<PushTask<Node>, CountingTrigger<Node>> split_closures
	final IDMap<K, TrackingTrigger<Node>> deflate_closures

	final Callback init_value_handler(k, v)

	// FIXME right types
	inflate_closures.put(root, new inflate_subnodes(null, map, null, null))
	inflated.push(root)

	do:

		while inflated not empty:
			task = inflated.pop()

			inflate_closures.remove(task).invoke(task.data)

		while deflated not empty:
			task = deflated.pop()

			clo = split_closures.remove(task)
N-			if clo.release() == 0:
				clo.invoke()

		while value_complete not empty:
			k = value_complete.pop()
			// TODO update value

			clo = deflate_closures.remove(k)
V-			if clo.release(k) == 0:
				clo.invoke()

	while (inflated + deflated + inflate_closures + split_closures + deflate_closures) not empty


Closure<Node> inflate_subnodes(
	Node parent
	SortedMap map
	CountingTrigger parent_node_clo
	TrackingTrigger parent_value_clo
	):

	invoke(Node node):
		assert(node.lkey < map.first && map.last < node.rkey)

		// a null trigger whose only purpose is to track unhandled (k,v) callbacks
		value_clo = new NullTrackingTrigger(parent)

		// closure to be called when all subnodes have been handled
		node_clo = new split_node(node, parent, value_clo, parent_node_clo, parent_value_clo)

		mergesort map & node:

			items added locally to the node:
V+				value_clo.acquire(k)
				deflate_closures.put(k, value_clo)
				init_value_handler(k, v)

			submap not in node:

				if node is leaf:
					as "added locally"

				else:
O+					node_clo.acquire()
					inflate_closures.put(task = new PullTask(n), new inflate_subnodes(node, submap, node_clo, value_clo))
					pull_queue.push(task)

		node_clo.close()

		if node_clo.count == 0:
			node_clo.invoke(node)


Runnable split_node(
	Node node
	Node parent
	TrackingTrigger value_clo
	CountingTrigger parent_node_clo
	TrackingTrigger parent_value_clo
	):

	run():

		// FIXME consider case where parent == null, node == root

		if size > node_max and parent != null:
			(split_nodes, separators) = node.split()
			splice into parent
		else:
			(split_nodes, separators) = ([this], [])

		for (k,v) in separators & value_clo:
			// for each separator key that was moved to the parent node, deassign
			// it from the current trigger and re-assign it to the parent trigger
			// NOTE: if the overall algorithm is ever made concurrent, this section
			// MUST be made atomic
S+			parent_value_clo.acquire(k)
S-			value_clo.release(k)
			old = deflate_closures.put(k, parent_value_clo)
			assert(old == value_clo)

		for n in split_nodes:
			// for each split node, create a trigger that will fire when all
			// its (k,v) pairs have been popped from value_complete
			vclo = new deflate_splitnode(n)
			for (k,v) in n & value_clo:
				// same deal as for the previous section
T+				vclo.acquire(k)
T-				value_clo.release(k)
				old = deflate_closures.put(k, vclo)
				assert(old == value_clo)
			vclo.close()

N+			parent_node_clo.acquire()
			split_closures.put(task = new PushTask(node), parent_node_clo)

		value_clo.close()
		assert(value_clo.count == 0)

		// original (unsplit) node acquired a ticket on parent_node_clo, release it now
O-		parent_node_clo.release()


Runnable deflate_splitnode(
	PushTask<Node> task
	):

	invoke():
		push_queue.push(task)


== Functionality ==

TODO sort this out

FetchTokenEntries [term]
ClearTokenEntries [term]

InsertTermEntry [data]
RemoveTermEntry [data]

FetchURIEntry [uri]
ClearURIEntry [uri]

InsertURIEntry [data]
RemoveURIEntry [data]

PullIndex [index]
PushIndex [index]






== Data elements ==

A "term" is a discrete phrase that results are associated with. This is just a
Java String, and can contain any character including whitespace.

A TermEntry stores data about a particular result for a term. Each entry has a
subject term and a relevance rating in [0,1]. Further types are:

 : TermTermEntry - redirects to another search term
 : TermIndexEntry - redirects to another index
 : TermPageEntry - final target for term

A URIKey is the node routing-key part of a FreenetURI and is used as the key
into the URI table. FreenetURIs from the same "site" will have the same URIKey
and will be stored close together in the table. (TODO: this might not be
necessary, especially if we use B+-trees instead of B-trees.)

A URIEntry stores data about a particular FreenetURI. Each entry has a quality
rating in [0,1]. Further types are:
 : etc? TODO

=== Unimplemented features ===

At the time of writing, there is no support for terms containing whitespace in
either XMLSpider or the syntax of the queries in the end-user interface.

There is also no support for URIEntrys in the spider or the interface.

== Data structure ==

 |-- contains
 :   subclass of

Index
 |-- metadata
 |-- ttab: BTreeMap<String, BTreeSet<TermEntry>>
 |-- utab: BTreeMap<URIKey, BTreeMap<FreenetURI, URIEntry>>

The index root would usually be stored as an SSK splitfile.

Conceptually, the ttab (term table) structure maps a term to (the collection of
entries for that term), and the utab (URI table) maps a FreenetURI to a single
unique URIEntry.

Structurally, both the ttab and utab are doubly-nested B-trees. There is a top
level B-tree, which maps a key to a further B-tree, which in turn holds data.

For the ttab, the top level B-tree maps a term to its entries collection, which
is a B-tree and can be nagivated. For the utab, the top level B-tree maps a
URIKey to a lower level B-tree that maps a FreenetURI to its URIEntry.


DOCUMENT usage of Bins


== Filters ==

TODO implement filters, think about how to do this...


